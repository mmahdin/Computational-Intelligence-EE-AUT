{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import ConcatDataset, random_split, DataLoader\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "# import pandas as pd\n",
    "\n",
    "# # Load the training dataset\n",
    "# train_csv = 'imdb_train.csv'  # Replace with actual path\n",
    "# train_data = pd.read_csv(train_csv)\n",
    "\n",
    "# # Count unique tokens in the 'tokenized' column\n",
    "# # Assuming 'tokenized' column contains lists of tokens as strings, e.g., \"[1, 23, 456]\"\n",
    "# all_tokens = []\n",
    "# for tokens in train_data['tokenized']:\n",
    "#     token_list = eval(tokens)  # Convert the string representation to a list\n",
    "#     all_tokens.extend(token_list)\n",
    "\n",
    "# # Calculate the vocabulary size\n",
    "# vocab_size = len(set(all_tokens))\n",
    "# print(vocab_size)\n",
    "\n",
    "# # Load the training dataset\n",
    "# train_csv = 'imdb_train.csv'  # Replace with actual path\n",
    "# train_data = pd.read_csv(train_csv)\n",
    "\n",
    "# # Find the maximum sequence length in the 'tokenized' column\n",
    "# # Assuming 'tokenized' column contains lists of tokens as strings, e.g., \"[1, 23, 456]\"\n",
    "# max_seq_length = max(len(eval(tokens)) for tokens in train_data['tokenized'])\n",
    "# print(\"Maximum Sequence Length:\", max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset class with padding\n",
    "class SentimentAnalysisDataset(Dataset):\n",
    "    def __init__(self, csv_file, max_length=2494, vocab_size=88585):\n",
    "        # Load data from CSV\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.max_length = max_length  # Set max length for padding\n",
    "        self.vocab_size = vocab_size  # Maximum vocab index\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the row by index\n",
    "        review = self.data.loc[idx, 'review']\n",
    "        tokenized = eval(self.data.loc[idx, 'tokenized'])  # assuming tokenized is stored as a string of list\n",
    "        label = self.data.loc[idx, 'label']\n",
    "        \n",
    "        # Ensure token indices are within the vocab_size range\n",
    "        tokenized = [min(token, self.vocab_size - 1) for token in tokenized]\n",
    "        \n",
    "        # Convert to tensor and pad, move to the appropriate device\n",
    "        tokenized_tensor = torch.tensor(tokenized, dtype=torch.long).to(device)\n",
    "        tokenized_tensor = F.pad(\n",
    "            tokenized_tensor, (0, self.max_length - len(tokenized_tensor)), value=0\n",
    "        )  # Pad with zeros up to max_length\n",
    "        \n",
    "        # Convert label to tensor and move to device\n",
    "        label_tensor = torch.tensor(label, dtype=torch.long).to(device)\n",
    "        \n",
    "        return tokenized_tensor, label_tensor\n",
    "\n",
    "# Paths to CSV files\n",
    "train_csv = 'imdb_train.csv'\n",
    "test_csv = 'imdb_test.csv'\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = SentimentAnalysisDataset(train_csv)\n",
    "test_dataset = SentimentAnalysisDataset(test_csv)\n",
    "\n",
    "# Combine train and test datasets\n",
    "combined_dataset = ConcatDataset([train_dataset, test_dataset])\n",
    "\n",
    "# Define split proportions\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "\n",
    "# Calculate dataset sizes\n",
    "train_size = int(train_ratio * len(combined_dataset))\n",
    "val_size = int(val_ratio * len(combined_dataset))\n",
    "test_size = len(combined_dataset) - train_size - val_size\n",
    "\n",
    "# Split combined dataset\n",
    "new_train_dataset, new_val_dataset, new_test_dataset = random_split(\n",
    "    combined_dataset, [train_size, val_size, test_size]\n",
    ")\n",
    "\n",
    "# Define desired subset sizes\n",
    "train_subset_size = 20000\n",
    "val_subset_size = 2000\n",
    "test_subset_size = 2000\n",
    "\n",
    "# Create smaller subsets of the original datasets\n",
    "new_train_dataset, _ = random_split(new_train_dataset, [train_subset_size, len(new_train_dataset) - train_subset_size])\n",
    "new_val_dataset, _ = random_split(new_val_dataset, [val_subset_size, len(new_val_dataset) - val_subset_size])\n",
    "new_test_dataset, _ = random_split(new_test_dataset, [test_subset_size, len(new_test_dataset) - test_subset_size])\n",
    "\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(new_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(new_val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(new_test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Text: torch.Size([2494])\n",
      "Label: tensor(1, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Print one data row from the training dataset\n",
    "sample_index = 0  # Replace with any valid index to print a different row\n",
    "tokenized_tensor, label_tensor = train_dataset[sample_index]\n",
    "\n",
    "print(\"Tokenized Text:\", tokenized_tensor.shape)\n",
    "print(\"Label:\", label_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "  <img src=\"image.png\" alt=\"image description\"/>\n",
    "</div>\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"image-1.png\" alt=\"image description\"/>\n",
    "</div>\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"image-2.png\" alt=\"image description\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the RNN model for sentiment analysis\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        \"\"\"\n",
    "        N: num batches (sentences).\n",
    "        D: Each token is represented by a D-dimensional embedding vector (embed_size).\n",
    "        T: Maximum sequence length (number of words in each sequence)\n",
    "        H: hidden_size\n",
    "\n",
    "        shape input: (N, T)\n",
    "        shape embeded input: (N, T, D)\n",
    "        \n",
    "        for each word in the sentence:\n",
    "            next_h = torch.tanh(x.mm(Wx) + prev_h.mm(Wh) + b)\n",
    "            {\n",
    "                where:\n",
    "                x:      (N,D)\n",
    "                Wx:     (D,H)\n",
    "                Wh:     (H,H)\n",
    "                b:      (H,)\n",
    "                next_h: (N,H)\n",
    "            }\n",
    "            (This is one step in the image above.)\n",
    "        \n",
    "        This process repeats for all the words in the sentence, so the number of output or hidden states at the end is T.\n",
    "\n",
    "        output: (N, T, H)\n",
    "        \"\"\"\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size) # assigne a vector of embec_size to each word\n",
    "        self.rnn = nn.RNN(embed_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 2)\n",
    "        self.hidden_dim = hidden_size\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        hidden: (N, H)\n",
    "        \"\"\"        \n",
    "        x = self.embedding(x)\n",
    "        output, hidden = self.rnn(x)\n",
    "        prediction = self.fc(output[:, -1, :]) \n",
    "        return prediction\n",
    "    \n",
    "\n",
    "# Define the RNN model for sentiment analysis\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \"\"\"\n",
    "        N: num batches (sentences).\n",
    "        D: Each token is represented by a D-dimensional embedding vector (embed_size).\n",
    "        T: Maximum sequence length (number of words in each sequence)\n",
    "        H: hidden_size\n",
    "\n",
    "        shape input: (N, T)\n",
    "        shape embeded input: (N, T, D)\n",
    "        \n",
    "        for each word in the sentence:\n",
    "            next_h = torch.tanh(x.mm(Wx) + prev_h.mm(Wh) + b)\n",
    "            {\n",
    "                where:\n",
    "                x:      (N,D)\n",
    "                Wx:     (D,H)\n",
    "                Wh:     (H,H)\n",
    "                b:      (H,)\n",
    "                next_h: (N,H)\n",
    "            }\n",
    "            (This is one step in the image above.)\n",
    "        \n",
    "        This process repeats for all the words in the sentence, so the number of output or hidden states at the end is T.\n",
    "\n",
    "        output: (N, T, H)\n",
    "        \"\"\"\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size) # assigne a vector of embec_size to each word\n",
    "        self.rnn = nn.RNN(embed_size, hidden_size, batch_first=True)\n",
    "\n",
    "        self.hidden_dim = hidden_size\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        hidden: (N, H)\n",
    "        \"\"\"        \n",
    "        x = self.embedding(x)\n",
    "        output, hidden = self.rnn(x)\n",
    "        return hidden\n",
    "    \n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, output_dim, hidden_dim):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.rnn = nn.RNN(hidden_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "    def forward(self, hidden):\n",
    "        batch_size = hidden.size(1)\n",
    "        input = torch.zeros(batch_size, 1, self.hidden_dim).to(hidden.device)  # [batch_size, 1, hidden_dim]\n",
    "        outputs, hidden = self.rnn(input, hidden)\n",
    "        \n",
    "        # Pass final RNN output to linear layer\n",
    "        prediction = self.fc(outputs.squeeze(1))  # prediction = [batch_size, output_dim]\n",
    "        return self.sigmoid(prediction).squeeze() \n",
    "    \n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, src):\n",
    "        hidden = self.encoder(src)\n",
    "        output = self.decoder(hidden)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********* train model **********\n",
      "Epoch [1/5], Loss: 0.6936, Val Accuracy: 0.5030\n",
      "Epoch [2/5], Loss: 0.6931, Val Accuracy: 0.5030\n",
      "Epoch [3/5], Loss: 0.6931, Val Accuracy: 0.5030\n",
      "Epoch [4/5], Loss: 0.6931, Val Accuracy: 0.5030\n",
      "Epoch [5/5], Loss: 0.6931, Val Accuracy: 0.4970\n",
      "********* evaluate model **********\n",
      "Test Loss: 0.6931\n",
      "Test Accuracy: 0.4810\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.00      0.00      0.00      1038\n",
      "    Positive       0.48      1.00      0.65       962\n",
      "\n",
      "    accuracy                           0.48      2000\n",
      "   macro avg       0.24      0.50      0.32      2000\n",
      "weighted avg       0.23      0.48      0.31      2000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mahdi/miniconda3/envs/ml2/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/mahdi/miniconda3/envs/ml2/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/mahdi/miniconda3/envs/ml2/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Set parameters\n",
    "vocab_size = 88585  # As per your dataset\n",
    "embed_size = 64\n",
    "hidden_size = 256\n",
    "output_size = 2  # Assuming binary classification: positive or negative sentiment\n",
    "learning_rate = 0.01\n",
    "\n",
    "\n",
    "# Instantiate the model, define the loss function and optimizer\n",
    "encoder = EncoderRNN(vocab_size, embed_size, hidden_size)\n",
    "decoder = DecoderRNN(output_size, hidden_size)\n",
    "model = Seq2Seq(encoder, decoder).to(device)\n",
    "# model = SimpleRNN(vocab_size, embed_size, hidden_size).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=2):\n",
    "    print('********* train model **********')\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Compute loss and backward pass\n",
    "            loss = criterion(outputs, labels.long())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # Validation phase after each epoch\n",
    "        model.eval()\n",
    "        all_preds, all_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(inputs)\n",
    "                \n",
    "                # Get predictions\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                all_preds.extend(preds.tolist())\n",
    "                all_labels.extend(labels.tolist())\n",
    "        \n",
    "        # Calculate validation accuracy\n",
    "        val_accuracy = accuracy_score(all_labels, all_preds)\n",
    "        \n",
    "        # Print epoch metrics\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss / len(train_loader):.4f}, Val Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, test_loader, criterion, device):\n",
    "    print('********* evaluate model **********')\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels.long())\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Get predictions\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.tolist())\n",
    "            all_labels.extend(labels.tolist())\n",
    "\n",
    "    # Calculate test accuracy\n",
    "    test_accuracy = accuracy_score(all_labels, all_preds)\n",
    "    \n",
    "    # Print classification report\n",
    "    report = classification_report(all_labels, all_preds, target_names=[\"Negative\", \"Positive\"])\n",
    "    print(f'Test Loss: {total_loss / len(test_loader):.4f}')\n",
    "    print(f'Test Accuracy: {test_accuracy:.4f}')\n",
    "    print(\"Classification Report:\\n\", report)\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=5)\n",
    "\n",
    "# Evaluate the model on test data\n",
    "evaluate_model(model, test_loader, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the model's state_dict (weights)\n",
    "# torch.save(model.state_dict(), 'model.pth')\n",
    "\n",
    "# # Optionally, you can also save the optimizer's state_dict if you want to resume training:\n",
    "# torch.save({\n",
    "#     'model_state_dict': model.state_dict(),\n",
    "#     'optimizer_state_dict': optimizer.state_dict(),\n",
    "#     'epoch': 20,\n",
    "#     'loss': 0.7,  # or any other info you want to save\n",
    "# }, 'checkpoint.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Training Loss: 0.6939677159309388\n",
      "Validation Loss: 0.6932293100962563, Validation Accuracy: 0.497\n",
      "Epoch 2/5, Training Loss: 0.6932436414718628\n",
      "Validation Loss: 0.6932528936673724, Validation Accuracy: 0.497\n",
      "Epoch 3/5, Training Loss: 0.6932362024307251\n",
      "Validation Loss: 0.6931363997005281, Validation Accuracy: 0.503\n",
      "Epoch 4/5, Training Loss: 0.6932073825836181\n",
      "Validation Loss: 0.6931328073380485, Validation Accuracy: 0.503\n",
      "Epoch 5/5, Training Loss: 0.693218869972229\n",
      "Validation Loss: 0.6932537432700868, Validation Accuracy: 0.497\n",
      "Test Accuracy: 0.481\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.00      0.00      0.00      1038\n",
      "    Positive       0.48      1.00      0.65       962\n",
      "\n",
      "    accuracy                           0.48      2000\n",
      "   macro avg       0.24      0.50      0.32      2000\n",
      "weighted avg       0.23      0.48      0.31      2000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mahdi/miniconda3/envs/ml2/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/mahdi/miniconda3/envs/ml2/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/mahdi/miniconda3/envs/ml2/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Define the LSTM model\n",
    "class SentimentLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=256, output_dim=2):\n",
    "        super(SentimentLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        _, (hidden, _) = self.lstm(embedded)\n",
    "        output = self.fc(hidden.squeeze(0))\n",
    "        return output\n",
    "\n",
    "# Model parameters\n",
    "vocab_size = 88585\n",
    "embedding_dim = 128\n",
    "hidden_dim = 256\n",
    "output_dim = 2\n",
    "\n",
    "# Instantiate the model, criterion, and optimizer\n",
    "model = SentimentLSTM(vocab_size, embedding_dim, hidden_dim, output_dim).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Training loop\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=5):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {train_loss / len(train_loader)}\")\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        val_accuracy = accuracy_score(all_labels, all_preds)\n",
    "        print(f\"Validation Loss: {val_loss / len(val_loader)}, Validation Accuracy: {val_accuracy}\")\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, epochs=5)\n",
    "\n",
    "# Testing and Evaluation\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Print classification report\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    report = classification_report(all_labels, all_preds, target_names=[\"Negative\", \"Positive\"])\n",
    "    print(f\"Test Accuracy: {accuracy}\")\n",
    "    print(\"Classification Report:\\n\", report)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "evaluate_model(model, test_loader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
